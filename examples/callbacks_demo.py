"""
Callbacks - ì´ë²¤íŠ¸ í•¸ë“¤ë§ ì‹œìŠ¤í…œ
ë¡œê¹…, ë¹„ìš© ì¶”ì , íƒ€ì´ë°, ìŠ¤íŠ¸ë¦¬ë° ë“±
"""
import time
from beanllm import (
    BaseCallback,
    LoggingCallback,
    CostTrackingCallback,
    TimingCallback,
    StreamingCallback,
    FunctionCallback,
    CallbackManager,
    create_callback_manager
)


def demo_logging_callback():
    """ë¡œê¹… ì½œë°±"""
    print("\n" + "="*60)
    print("1ï¸âƒ£  LoggingCallback - ë¡œê¹…")
    print("="*60)

    callback = LoggingCallback(verbose=True)

    # ì‹œë®¬ë ˆì´ì…˜
    print("\n[LLM í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜]")

    callback.on_llm_start("gpt-4o-mini", [{"role": "user", "content": "Hello"}])
    time.sleep(0.5)  # ì‹œë®¬ë ˆì´ì…˜
    callback.on_llm_end("gpt-4o-mini", "Hi there!", tokens_used=15)

    print("\n[Agent ì‹¤í–‰ ì‹œë®¬ë ˆì´ì…˜]")
    callback.on_agent_start("researcher", "Find information about AI")
    callback.on_agent_action("researcher", "Using search tool")
    time.sleep(0.3)
    callback.on_agent_end("researcher", "Found 5 results")

    print("\nğŸ’¡ ëª¨ë“  ì´ë²¤íŠ¸ë¥¼ ë¡œê·¸ë¡œ ì¶œë ¥!")


def demo_cost_tracking():
    """ë¹„ìš© ì¶”ì  ì½œë°±"""
    print("\n" + "="*60)
    print("2ï¸âƒ£  CostTrackingCallback - ë¹„ìš© ì¶”ì ")
    print("="*60)

    callback = CostTrackingCallback()

    # ì—¬ëŸ¬ LLM í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
    print("\n[LLM í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜]")

    # GPT-4o-mini (ì €ë ´)
    callback.on_llm_end(
        "gpt-4o-mini",
        "Response 1",
        input_tokens=100,
        output_tokens=50
    )
    print(f"  Call 1: gpt-4o-mini (100 in + 50 out)")

    # GPT-4o (ë¹„ìŒˆ)
    callback.on_llm_end(
        "gpt-4o",
        "Response 2",
        input_tokens=200,
        output_tokens=100
    )
    print(f"  Call 2: gpt-4o (200 in + 100 out)")

    # GPT-3.5-turbo
    callback.on_llm_end(
        "gpt-3.5-turbo",
        "Response 3",
        input_tokens=150,
        output_tokens=75
    )
    print(f"  Call 3: gpt-3.5-turbo (150 in + 75 out)")

    # í†µê³„
    print("\n[ë¹„ìš© í†µê³„]")
    stats = callback.get_stats()
    print(f"  ì´ í˜¸ì¶œ:        {stats['total_calls']}ë²ˆ")
    print(f"  ì´ ì…ë ¥ í† í°:   {stats['total_input_tokens']:,}")
    print(f"  ì´ ì¶œë ¥ í† í°:   {stats['total_output_tokens']:,}")
    print(f"  ì´ í† í°:        {stats['total_tokens']:,}")
    print(f"  ì´ ë¹„ìš©:        ${stats['total_cost']:.6f}")

    print("\n[í˜¸ì¶œë³„ ë¹„ìš©]")
    for i, call in enumerate(stats['calls'], 1):
        print(f"  {i}. {call['model']}: ${call['cost']:.6f}")

    print("\nğŸ’¡ LLM ì‚¬ìš© ë¹„ìš©ì„ ìë™ìœ¼ë¡œ ì¶”ì !")


def demo_timing_callback():
    """íƒ€ì´ë° ì¶”ì  ì½œë°±"""
    print("\n" + "="*60)
    print("3ï¸âƒ£  TimingCallback - ì‹¤í–‰ ì‹œê°„ ì¶”ì ")
    print("="*60)

    callback = TimingCallback()

    # ì—¬ëŸ¬ í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
    print("\n[LLM í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜]")

    for i in range(3):
        callback.on_llm_start("gpt-4o-mini", [{"role": "user", "content": f"Query {i+1}"}])
        time.sleep(0.1 * (i + 1))  # ê°ê¸° ë‹¤ë¥¸ ì‹œê°„
        callback.on_llm_end("gpt-4o-mini", f"Response {i+1}")
        print(f"  Call {i+1} completed")

    # í†µê³„
    print("\n[íƒ€ì´ë° í†µê³„]")
    stats = callback.get_stats()
    print(f"  ì´ í˜¸ì¶œ:      {stats['total_calls']}ë²ˆ")
    print(f"  ì´ ì‹œê°„:      {stats['total_time']:.3f}ì´ˆ")
    print(f"  í‰ê·  ì‹œê°„:    {stats['average_time']:.3f}ì´ˆ")
    print(f"  ìµœì†Œ ì‹œê°„:    {stats['min_time']:.3f}ì´ˆ")
    print(f"  ìµœëŒ€ ì‹œê°„:    {stats['max_time']:.3f}ì´ˆ")

    print("\n[í˜¸ì¶œë³„ ì‹œê°„]")
    for i, timing in enumerate(stats['timings'], 1):
        print(f"  {i}. {timing['model']}: {timing['duration']:.3f}ì´ˆ")

    print("\nğŸ’¡ ê° í˜¸ì¶œì˜ ì‹¤í–‰ ì‹œê°„ì„ ì¸¡ì •!")


def demo_streaming_callback():
    """ìŠ¤íŠ¸ë¦¬ë° ì½œë°±"""
    print("\n" + "="*60)
    print("4ï¸âƒ£  StreamingCallback - ìŠ¤íŠ¸ë¦¬ë°")
    print("="*60)

    # ê°„ë‹¨í•œ í† í° ì¶œë ¥
    print("\n[ë°©ë²• 1: ì¦‰ì‹œ ì¶œë ¥]")
    print("  Output: ", end="")

    callback = StreamingCallback(
        on_token=lambda token: print(token, end="", flush=True)
    )

    # í† í° ì‹œë®¬ë ˆì´ì…˜
    tokens = ["Hello", " ", "World", "!", " ", "How", " ", "are", " ", "you", "?"]
    for token in tokens:
        callback.on_llm_token(token)
        time.sleep(0.05)

    callback.on_llm_end("gpt-4o-mini", "")
    print()

    # ë²„í¼ë§
    print("\n[ë°©ë²• 2: ë²„í¼ë§ (3í† í°ì”©)]")
    print("  Output: ", end="")

    buffered_tokens = []

    def buffer_token(text: str):
        buffered_tokens.append(text)
        print(f"[{text}]", end=" ", flush=True)

    callback = StreamingCallback(
        on_token=buffer_token,
        buffer_size=3
    )

    for token in tokens:
        callback.on_llm_token(token)
        time.sleep(0.05)

    callback.on_llm_end("gpt-4o-mini", "")
    print()

    print(f"\n  ë²„í¼ë§ëœ ì²­í¬: {len(buffered_tokens)}ê°œ")

    print("\nğŸ’¡ í† í°ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì²˜ë¦¬!")


def demo_function_callback():
    """í•¨ìˆ˜ ê¸°ë°˜ ì½œë°±"""
    print("\n" + "="*60)
    print("5ï¸âƒ£  FunctionCallback - ì»¤ìŠ¤í…€ í•¨ìˆ˜")
    print("="*60)

    # ê°„ë‹¨í•œ í•¸ë“¤ëŸ¬
    print("\n[ì»¤ìŠ¤í…€ í•¸ë“¤ëŸ¬]")

    callback = FunctionCallback(
        on_start=lambda model, **kw: print(f"  ğŸš€ Starting: {model}"),
        on_end=lambda model, response, **kw: print(f"  âœ… Finished: {model}"),
        on_error=lambda model, error, **kw: print(f"  âŒ Error in {model}: {error}")
    )

    # ì‹œë®¬ë ˆì´ì…˜
    callback.on_llm_start("gpt-4o-mini", [])
    time.sleep(0.2)
    callback.on_llm_end("gpt-4o-mini", "Response")

    # ì—ëŸ¬
    try:
        raise ValueError("Test error")
    except Exception as e:
        callback.on_llm_error("gpt-4o", e)

    print("\nğŸ’¡ ê°„ë‹¨í•œ í•¨ìˆ˜ë¡œ ì»¤ìŠ¤í…€ ì½œë°±!")


def demo_custom_callback():
    """ì»¤ìŠ¤í…€ ì½œë°± í´ë˜ìŠ¤"""
    print("\n" + "="*60)
    print("6ï¸âƒ£  ì»¤ìŠ¤í…€ Callback í´ë˜ìŠ¤")
    print("="*60)

    class MyCustomCallback(BaseCallback):
        """ì»¤ìŠ¤í…€ ì½œë°± ì˜ˆì œ"""

        def __init__(self):
            self.events = []

        def on_llm_start(self, model: str, messages, **kwargs):
            event = f"LLM Start: {model}"
            self.events.append(event)
            print(f"  ğŸ“ {event}")

        def on_llm_end(self, model: str, response: str, **kwargs):
            event = f"LLM End: {model}"
            self.events.append(event)
            print(f"  ğŸ“ {event}")

        def on_agent_action(self, agent_name: str, action: str, **kwargs):
            event = f"Agent Action: {agent_name} - {action}"
            self.events.append(event)
            print(f"  ğŸ“ {event}")

        def get_event_count(self) -> int:
            return len(self.events)

    # ì‚¬ìš©
    print("\n[ì»¤ìŠ¤í…€ ì½œë°± ì‚¬ìš©]")
    callback = MyCustomCallback()

    callback.on_llm_start("gpt-4o-mini", [])
    callback.on_agent_action("researcher", "searching")
    callback.on_llm_end("gpt-4o-mini", "Done")

    print(f"\n  ì´ ì´ë²¤íŠ¸: {callback.get_event_count()}ê°œ")

    print("\nğŸ’¡ BaseCallbackì„ ìƒì†í•˜ì—¬ ì™„ì „ ì»¤ìŠ¤í…€!")


def demo_callback_manager():
    """ì½œë°± ë§¤ë‹ˆì € - ì—¬ëŸ¬ ì½œë°± í•œ ë²ˆì—"""
    print("\n" + "="*60)
    print("7ï¸âƒ£  CallbackManager - ì—¬ëŸ¬ ì½œë°± ê´€ë¦¬")
    print("="*60)

    # ì—¬ëŸ¬ ì½œë°± ìƒì„±
    logging_cb = LoggingCallback(verbose=True)
    cost_cb = CostTrackingCallback()
    timing_cb = TimingCallback()

    # ë§¤ë‹ˆì €ë¡œ ê´€ë¦¬
    manager = create_callback_manager(
        logging_cb,
        cost_cb,
        timing_cb
    )

    # í•œ ë²ˆì— íŠ¸ë¦¬ê±°
    print("\n[LLM í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜]")

    manager.on_llm_start("gpt-4o-mini", [{"role": "user", "content": "Test"}])
    time.sleep(0.3)
    manager.on_llm_end(
        "gpt-4o-mini",
        "Response",
        input_tokens=100,
        output_tokens=50
    )

    # ê° ì½œë°± í†µê³„
    print("\n[ë¹„ìš©]")
    print(f"  ì´ ë¹„ìš©: ${cost_cb.get_total_cost():.6f}")

    print("\n[íƒ€ì´ë°]")
    timing_stats = timing_cb.get_stats()
    print(f"  í‰ê·  ì‹œê°„: {timing_stats['average_time']:.3f}ì´ˆ")

    print("\nğŸ’¡ ì—¬ëŸ¬ ì½œë°±ì„ í•œ ë²ˆì— ê´€ë¦¬í•˜ê³  íŠ¸ë¦¬ê±°!")


def demo_practical_usage():
    """ì‹¤ì „ ì‚¬ìš© ì˜ˆì œ"""
    print("\n" + "="*60)
    print("8ï¸âƒ£  ì‹¤ì „ ì‚¬ìš© - Clientì™€ í†µí•©")
    print("="*60)

    print("\n[ë°©ë²• 1: Clientì— ì§ì ‘ ì „ë‹¬]")
    print("""
    from beanllm import Client, LoggingCallback, CostTrackingCallback

    callbacks = [
        LoggingCallback(),
        CostTrackingCallback()
    ]

    client = Client(model="gpt-4o-mini", callbacks=callbacks)

    # ì‚¬ìš© - ìë™ìœ¼ë¡œ ì½œë°± í˜¸ì¶œë¨
    response = client.chat("Hello")
    """)

    print("\n[ë°©ë²• 2: CallbackManager ì‚¬ìš©]")
    print("""
    from beanllm import Client, create_callback_manager
    from beanllm import LoggingCallback, CostTrackingCallback

    manager = create_callback_manager(
        LoggingCallback(),
        CostTrackingCallback()
    )

    client = Client(model="gpt-4o-mini", callback_manager=manager)

    response = client.chat("Hello")
    """)

    print("\n[ë°©ë²• 3: ì‹¤í–‰ ì¤‘ ì½œë°± ì¶”ê°€]")
    print("""
    client = Client(model="gpt-4o-mini")

    # ì‹¤í–‰ ì¤‘ ì½œë°± ì¶”ê°€
    client.add_callback(LoggingCallback())
    client.add_callback(CostTrackingCallback())

    response = client.chat("Hello")
    """)

    print("\nğŸ’¡ Client, Agent, Chain ë“± ëª¨ë“  ê³³ì—ì„œ ì‚¬ìš© ê°€ëŠ¥!")


def main():
    """ëª¨ë“  ë°ëª¨ ì‹¤í–‰"""
    print("="*60)
    print("ğŸš€ Callbacks - ì´ë²¤íŠ¸ í•¸ë“¤ë§ ì‹œìŠ¤í…œ")
    print("="*60)
    print("\n8ê°€ì§€ ì½œë°± íƒ€ì…:")
    print("  1. LoggingCallback - ë¡œê¹…")
    print("  2. CostTrackingCallback - ë¹„ìš© ì¶”ì ")
    print("  3. TimingCallback - ì‹¤í–‰ ì‹œê°„")
    print("  4. StreamingCallback - ìŠ¤íŠ¸ë¦¬ë°")
    print("  5. FunctionCallback - ì»¤ìŠ¤í…€ í•¨ìˆ˜")
    print("  6. ì»¤ìŠ¤í…€ Callback í´ë˜ìŠ¤")
    print("  7. CallbackManager - ì—¬ëŸ¬ ì½œë°± ê´€ë¦¬")
    print("  8. ì‹¤ì „ ì‚¬ìš©ë²•")

    demo_logging_callback()
    demo_cost_tracking()
    demo_timing_callback()
    demo_streaming_callback()
    demo_function_callback()
    demo_custom_callback()
    demo_callback_manager()
    demo_practical_usage()

    print("\n" + "="*60)
    print("ğŸ‰ Callbacks ë°ëª¨ ì™„ë£Œ!")
    print("="*60)
    print("\nâœ¨ í•µì‹¬ ê¸°ëŠ¥:")
    print("  ë‚´ì¥ ì½œë°±:")
    print("    â€¢ LoggingCallback - ì´ë²¤íŠ¸ ë¡œê¹…")
    print("    â€¢ CostTrackingCallback - ë¹„ìš© ê³„ì‚°")
    print("    â€¢ TimingCallback - ì‹¤í–‰ ì‹œê°„")
    print("    â€¢ StreamingCallback - ì‹¤ì‹œê°„ ì²˜ë¦¬")
    print("\n  ì»¤ìŠ¤í…€:")
    print("    â€¢ FunctionCallback - ê°„ë‹¨í•œ í•¨ìˆ˜")
    print("    â€¢ BaseCallback ìƒì† - ì™„ì „ ì œì–´")
    print("\n  ê´€ë¦¬:")
    print("    â€¢ CallbackManager - ì—¬ëŸ¬ ì½œë°± í•œ ë²ˆì—")
    print("\n  ì´ë²¤íŠ¸:")
    print("    â€¢ on_llm_start/end/error/token")
    print("    â€¢ on_agent_start/end/error/action")
    print("    â€¢ on_chain_start/end/error")
    print("    â€¢ on_tool_start/end/error")
    print("\nğŸ’¡ ëª¨ë“  ì‹¤í–‰ ì´ë²¤íŠ¸ë¥¼ ì‰½ê²Œ ì¶”ì í•˜ê³  ì²˜ë¦¬!")


if __name__ == "__main__":
    main()
