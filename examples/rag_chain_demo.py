"""
RAG Chain - í•œ ì¤„ë¡œ ì™„ì „í•œ RAG ì‹œìŠ¤í…œ
ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ë¶€í„° ê³ ê¸‰ ì‚¬ìš©ê¹Œì§€
"""
from pathlib import Path
from beanllm import (
    RAGChain,
    RAGBuilder,
    create_rag,
    RAG,  # ì§§ì€ ë³„ì¹­
    DocumentLoader,
    TextSplitter,
    Embedding,
    from_documents,
    Client
)


def demo_simplest():
    """ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²• - í•œ ì¤„!"""
    print("\n" + "="*60)
    print("1ï¸âƒ£  ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²• - create_rag()")
    print("="*60)

    # í…ŒìŠ¤íŠ¸ íŒŒì¼
    test_file = Path("rag_test.txt")
    test_file.write_text("""
Artificial Intelligence (AI) is transforming technology.
Machine Learning (ML) is a subset of AI that learns from data.
Deep Learning (DL) uses neural networks with multiple layers.
Natural Language Processing (NLP) helps computers understand human language.
Computer Vision enables machines to interpret visual information.
    """.strip(), encoding="utf-8")

    try:
        print("\n[í•œ ì¤„ë¡œ RAG ìƒì„±]")

        # ê°€ì¥ ê°„ë‹¨! (API í‚¤ í•„ìš”)
        # rag = create_rag("rag_test.txt")

        # ë”ë¯¸ ì„ë² ë”©ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ (API í‚¤ ì—†ì´)
        import random
        embed_func = lambda texts: [[random.random() for _ in range(384)] for _ in texts]

        # ìˆ˜ë™ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ êµ¬ì„± (ë°ëª¨ìš©)
        docs = DocumentLoader.load(test_file)
        chunks = TextSplitter.split(docs, chunk_size=200)
        store = from_documents(chunks, embed_func)

        # RAG Chain ìƒì„±
        rag = RAGChain(
            vector_store=store,
            llm=Client(model="gpt-4o-mini")
        )

        print(f"  âœ“ RAG ìƒì„± ì™„ë£Œ")
        print(f"  âœ“ ë¬¸ì„œ: {len(docs)}ê°œ")
        print(f"  âœ“ ì²­í¬: {len(chunks)}ê°œ")

        # ì§ˆë¬¸í•˜ê¸° (ì‹¤ì œë¡œëŠ” LLM í˜¸ì¶œí•˜ì§€ ì•ŠìŒ - API í‚¤ í•„ìš”)
        print("\n[ì§ˆë¬¸ ì˜ˆì‹œ]")
        print("  Q: What is Machine Learning?")
        print("  A: (LLM ë‹µë³€ì´ ì—¬ê¸° ë‚˜ì˜´)")

        # ê²€ìƒ‰ë§Œ í…ŒìŠ¤íŠ¸
        print("\n[ê²€ìƒ‰ í…ŒìŠ¤íŠ¸]")
        results = rag.retrieve("Machine Learning", k=2)
        print(f"  âœ“ ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
        for i, r in enumerate(results, 1):
            print(f"  {i}. {r.document.content[:60]}...")

    except Exception as e:
        print(f"  âš ï¸  {e}")

    finally:
        if test_file.exists():
            test_file.unlink()

    print("\nğŸ’¡ create_rag(íŒŒì¼ê²½ë¡œ)ë§Œ ìˆìœ¼ë©´ ë!")


def demo_from_documents():
    """from_documents - ì§ì ‘ êµ¬ì„±"""
    print("\n" + "="*60)
    print("2ï¸âƒ£  RAGChain.from_documents()")
    print("="*60)

    test_file = Path("rag_test2.txt")
    test_file.write_text("""
Python is a high-level programming language.
JavaScript is essential for web development.
Java is widely used in enterprise applications.
C++ offers high performance for system programming.
Go is designed for concurrent programming.
    """.strip(), encoding="utf-8")

    try:
        import random
        embed_func = lambda texts: [[random.random() for _ in range(384)] for _ in texts]

        print("\n[from_documentsë¡œ RAG ìƒì„±]")

        # í•œ ë²ˆì— ìƒì„± (ë‚´ë¶€ì—ì„œ ìë™ìœ¼ë¡œ ëª¨ë“  ë‹¨ê³„ ìˆ˜í–‰)
        docs = DocumentLoader.load(test_file)
        chunks = TextSplitter.split(docs)
        store = from_documents(chunks, embed_func)

        rag = RAGChain(
            vector_store=store,
            llm=Client(model="gpt-4o-mini"),
            prompt_template="""Answer based on the context.

Context:
{context}

Question: {question}

Answer:"""
        )

        print(f"  âœ“ RAG ìƒì„± ì™„ë£Œ")
        print(f"  âœ“ ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©")

        # ê²€ìƒ‰ ì˜µì…˜ë“¤
        print("\n[ë‹¤ì–‘í•œ ê²€ìƒ‰ ì˜µì…˜]")

        print("  1. ì¼ë°˜ ê²€ìƒ‰:")
        results = rag.retrieve("Python", k=2)
        print(f"     {len(results)}ê°œ ê²°ê³¼")

        print("  2. MMR ê²€ìƒ‰ (ë‹¤ì–‘ì„±):")
        results = rag.retrieve("programming", k=3, mmr=True)
        print(f"     {len(results)}ê°œ ê²°ê³¼")

        # Hybrid searchëŠ” íŠ¹ì • vector storeì—ì„œë§Œ ì§€ì›
        print("  3. Hybrid ê²€ìƒ‰ (ë²¡í„°+í‚¤ì›Œë“œ):")
        try:
            results = rag.retrieve("language", k=2, hybrid=True)
            print(f"     {len(results)}ê°œ ê²°ê³¼")
        except Exception as e:
            print(f"     âš ï¸  {e}")

    except Exception as e:
        print(f"  âš ï¸  {e}")

    finally:
        if test_file.exists():
            test_file.unlink()

    print("\nğŸ’¡ ê²€ìƒ‰ ì˜µì…˜: k, rerank, mmr, hybrid")


def demo_builder():
    """RAGBuilder - Fluent API"""
    print("\n" + "="*60)
    print("3ï¸âƒ£  RAGBuilder - Fluent API")
    print("="*60)

    test_file = Path("rag_test3.txt")
    test_file.write_text("""
Quantum computing uses quantum mechanics principles.
Blockchain provides decentralized data storage.
IoT connects physical devices to the internet.
Cloud computing delivers services over the internet.
Edge computing processes data near the source.
    """.strip(), encoding="utf-8")

    try:
        import random
        embed_func = lambda texts: [[random.random() for _ in range(384)] for _ in texts]

        print("\n[Fluent APIë¡œ RAG êµ¬ì„±]")

        # Builder íŒ¨í„´
        rag = (RAGBuilder()
            .load_documents(test_file)
            .split_text(chunk_size=100, chunk_overlap=20)
            .embed_with(Embedding(model="text-embedding-3-small",
                                 provider="openai"))
            .use_llm(Client(model="gpt-4o"))
            .with_prompt("""Based on the context, provide a detailed answer.

Context:
{context}

Question: {question}

Detailed Answer:""")
            .build())

        print(f"  âœ“ RAG ë¹Œë“œ ì™„ë£Œ")
        print(f"  âœ“ ì„¸ë°€í•œ ì œì–´ ê°€ëŠ¥")

    except Exception as e:
        print(f"  âš ï¸  {e}")

    finally:
        if test_file.exists():
            test_file.unlink()

    print("\nğŸ’¡ Builder íŒ¨í„´ìœ¼ë¡œ ê° ë‹¨ê³„ ì œì–´")


def demo_query_methods():
    """ë‹¤ì–‘í•œ ì¿¼ë¦¬ ë©”ì„œë“œ"""
    print("\n" + "="*60)
    print("4ï¸âƒ£  ë‹¤ì–‘í•œ ì¿¼ë¦¬ ë©”ì„œë“œ")
    print("="*60)

    test_file = Path("rag_test4.txt")
    test_file.write_text("""
Data Science combines statistics and programming.
Big Data handles large volumes of data.
Data Mining extracts patterns from data.
Data Visualization presents data graphically.
Data Engineering builds data pipelines.
    """.strip(), encoding="utf-8")

    try:
        import random
        embed_func = lambda texts: [[random.random() for _ in range(384)] for _ in texts]

        docs = DocumentLoader.load(test_file)
        chunks = TextSplitter.split(docs)
        store = from_documents(chunks, embed_func)

        rag = RAGChain(
            vector_store=store,
            llm=Client(model="gpt-4o-mini")
        )

        print("\n[1] query() - ê¸°ë³¸ ì¿¼ë¦¬:")
        print("  answer = rag.query('What is data science?')")
        print("  # Returns: str")

        print("\n[2] query() with sources:")
        print("  answer, sources = rag.query('question', include_sources=True)")
        print("  # Returns: (str, List[VectorSearchResult])")

        print("\n[3] stream_query() - ìŠ¤íŠ¸ë¦¬ë°:")
        print("  for chunk in rag.stream_query('question'):")
        print("      print(chunk, end='')")

        print("\n[4] batch_query() - ë°°ì¹˜ ì²˜ë¦¬:")
        print("  questions = ['Q1', 'Q2', 'Q3']")
        print("  answers = rag.batch_query(questions)")
        print("  # Returns: List[str]")

        print("\n[5] aquery() - ë¹„ë™ê¸°:")
        print("  answer = await rag.aquery('question')")

        # ê²€ìƒ‰ë§Œ í…ŒìŠ¤íŠ¸
        print("\n[ê²€ìƒ‰ ê²°ê³¼ í™•ì¸]")
        results = rag.retrieve("Data Science", k=2)
        print(f"  âœ“ {len(results)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")
        for i, r in enumerate(results, 1):
            print(f"  {i}. {r.document.content[:50]}...")

    except Exception as e:
        print(f"  âš ï¸  {e}")

    finally:
        if test_file.exists():
            test_file.unlink()

    print("\nğŸ’¡ 5ê°€ì§€ ì¿¼ë¦¬ ë©”ì„œë“œ ì œê³µ")


def demo_advanced_retrieval():
    """ê³ ê¸‰ ê²€ìƒ‰ ê¸°ëŠ¥"""
    print("\n" + "="*60)
    print("5ï¸âƒ£  ê³ ê¸‰ ê²€ìƒ‰ ê¸°ëŠ¥")
    print("="*60)

    test_file = Path("rag_test5.txt")
    test_file.write_text("""
Neural networks learn patterns from data.
Convolutional networks excel at image tasks.
Recurrent networks handle sequential data.
Transformer networks use attention mechanisms.
Generative networks create new content.
    """.strip(), encoding="utf-8")

    try:
        import random
        embed_func = lambda texts: [[random.random() for _ in range(384)] for _ in texts]

        docs = DocumentLoader.load(test_file)
        chunks = TextSplitter.split(docs)
        store = from_documents(chunks, embed_func)

        rag = RAGChain(
            vector_store=store,
            llm=Client(model="gpt-4o-mini")
        )

        print("\n[ê²€ìƒ‰ ì „ëµ]")

        print("  1. ê¸°ë³¸ ê²€ìƒ‰ (ìœ ì‚¬ë„):")
        results = rag.retrieve("neural networks", k=3)
        print(f"     {len(results)}ê°œ ê²°ê³¼")

        print("\n  2. MMR (ë‹¤ì–‘ì„± ê³ ë ¤):")
        results = rag.retrieve("networks", k=3, mmr=True)
        print(f"     {len(results)}ê°œ ê²°ê³¼ (ì¤‘ë³µ ì œê±°)")

        print("\n  3. Re-ranking (ì •í™•ë„ í–¥ìƒ):")
        try:
            results = rag.retrieve("networks", k=3, rerank=True)
            print(f"     {len(results)}ê°œ ê²°ê³¼ (ì¬ìˆœìœ„í™”)")
        except ImportError:
            print("     âš ï¸  sentence-transformers í•„ìš”")

        print("\n  4. Hybrid (ë²¡í„° + í‚¤ì›Œë“œ):")
        try:
            results = rag.retrieve("neural", k=3, hybrid=True)
            print(f"     {len(results)}ê°œ ê²°ê³¼")
        except Exception as e:
            print(f"     âš ï¸  {e}")

        print("\n  5. ëª¨ë“  ì˜µì…˜ ì¡°í•©:")
        print("     retrieve(query, k=10, rerank=True, mmr=True)")

    except Exception as e:
        print(f"  âš ï¸  {e}")

    finally:
        if test_file.exists():
            test_file.unlink()

    print("\nğŸ’¡ ê²€ìƒ‰ í’ˆì§ˆì„ ë†’ì´ëŠ” ë‹¤ì–‘í•œ ì „ëµ")


def demo_aliases():
    """ë³„ì¹­ ì‚¬ìš©"""
    print("\n" + "="*60)
    print("6ï¸âƒ£  í¸ë¦¬í•œ ë³„ì¹­")
    print("="*60)

    print("\n[ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²•ë“¤]")
    print("""
    # 1. RAGChain (ì „ì²´ ì´ë¦„)
    rag = RAGChain.from_documents("doc.pdf")

    # 2. RAG (ì§§ì€ ë³„ì¹­)
    rag = RAG.from_documents("doc.pdf")

    # 3. create_rag (í•¨ìˆ˜)
    rag = create_rag("doc.pdf")

    # ëª¨ë‘ ë™ì¼í•œ ê²°ê³¼!
    """)

    print("ğŸ’¡ ì·¨í–¥ì— ë§ê²Œ ì„ íƒí•˜ì„¸ìš”")


def main():
    """ëª¨ë“  ë°ëª¨ ì‹¤í–‰"""
    print("="*60)
    print("ğŸš€ RAG Chain - í•œ ì¤„ë¡œ ì™„ì „í•œ RAG")
    print("="*60)
    print("\n6ê°€ì§€ ì‚¬ìš© ë°©ë²•:")
    print("  1. create_rag() - ê°€ì¥ ê°„ë‹¨")
    print("  2. RAGChain.from_documents() - ì§ì ‘ êµ¬ì„±")
    print("  3. RAGBuilder - Fluent API")
    print("  4. ë‹¤ì–‘í•œ ì¿¼ë¦¬ ë©”ì„œë“œ")
    print("  5. ê³ ê¸‰ ê²€ìƒ‰ ê¸°ëŠ¥")
    print("  6. í¸ë¦¬í•œ ë³„ì¹­")

    demo_simplest()
    demo_from_documents()
    demo_builder()
    demo_query_methods()
    demo_advanced_retrieval()
    demo_aliases()

    print("\n" + "="*60)
    print("ğŸ‰ RAG Chain ë°ëª¨ ì™„ë£Œ!")
    print("="*60)
    print("\nâœ¨ í•µì‹¬ ê¸°ëŠ¥:")
    print("  ìƒì„±:")
    print("    â€¢ create_rag(source) - ê°€ì¥ ê°„ë‹¨")
    print("    â€¢ RAGChain.from_documents(source) - ì œì–´")
    print("    â€¢ RAGBuilder().load()....build() - Fluent")
    print("\n  ì¿¼ë¦¬:")
    print("    â€¢ query() - ê¸°ë³¸")
    print("    â€¢ stream_query() - ìŠ¤íŠ¸ë¦¬ë°")
    print("    â€¢ batch_query() - ë°°ì¹˜")
    print("    â€¢ aquery() - ë¹„ë™ê¸°")
    print("\n  ê²€ìƒ‰:")
    print("    â€¢ retrieve(k=4, rerank=True, mmr=True, hybrid=True)")
    print("\nğŸ’¡ í•œ ì¤„ë¡œ ì™„ì „í•œ RAG ì‹œìŠ¤í…œ!")


if __name__ == "__main__":
    main()
