"""
ê°œì„ ëœ API ë°ëª¨ - ì‚¬ìš©ìê°€ ì‰½ê²Œ ì„¤ì •í•˜ê³  ì¡°ì •í•  ìˆ˜ ìˆëŠ” ë°©ë²•
"""
from pathlib import Path
from beanllm import DocumentLoader, TextSplitter, Document


def demo_loader_type_selection():
    """DocumentLoader - ëª…ì‹œì  íƒ€ì… ì§€ì •"""
    print("\n" + "="*60)
    print("ğŸ“‚ DocumentLoader - íƒ€ì… ì§€ì • ë°ëª¨")
    print("="*60)

    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
    test_txt = Path("test.txt")
    test_txt.write_text("Text file content", encoding="utf-8")

    test_csv = Path("test.csv")
    test_csv.write_text("name,value\nAlice,100\nBob,200", encoding="utf-8")

    try:
        # ë°©ë²• 1: ìë™ ê°ì§€ (ê¸°ë³¸)
        print("\n1. ìë™ ê°ì§€ (ê¸°ë³¸):")
        docs = DocumentLoader.load("test.txt")
        print(f"   âœ“ ìë™ ê°ì§€: {len(docs)} ë¬¸ì„œ ë¡œë”©")

        # ë°©ë²• 2: ëª…ì‹œì  íƒ€ì… ì§€ì •
        print("\n2. ëª…ì‹œì  íƒ€ì… ì§€ì •:")
        docs_text = DocumentLoader.load("test.txt", loader_type="text")
        print(f"   âœ“ loader_type='text': {len(docs_text)} ë¬¸ì„œ")

        docs_csv = DocumentLoader.load("test.csv", loader_type="csv")
        print(f"   âœ“ loader_type='csv': {len(docs_csv)} ë¬¸ì„œ")

        # ë°©ë²• 3: íƒ€ì… ì§€ì • + ì¶”ê°€ íŒŒë¼ë¯¸í„°
        print("\n3. íƒ€ì… + íŒŒë¼ë¯¸í„°:")
        docs_custom = DocumentLoader.load(
            "test.csv",
            loader_type="csv",
            content_columns=["name"]
        )
        print(f"   âœ“ CSV íŠ¹ì • ì»¬ëŸ¼ë§Œ: {docs_custom[0].content}")

        print("\nâœ“ DocumentLoader: ìë™ ê°ì§€ + ëª…ì‹œì  ì„ íƒ ë‘˜ ë‹¤ ê°€ëŠ¥!")

    finally:
        # ì •ë¦¬
        test_txt.unlink()
        test_csv.unlink()


def demo_splitter_strategies():
    """TextSplitter - ì‰¬ìš´ ì „ëµ ì„ íƒ"""
    print("\n" + "="*60)
    print("âœ‚ï¸  TextSplitter - ì „ëµ ì„ íƒ ë°ëª¨")
    print("="*60)

    # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ
    text = """
# AI Overview

Artificial Intelligence is transforming the world.

## Machine Learning

Machine learning algorithms learn from data.

## Deep Learning

Deep learning uses neural networks.
    """.strip()

    docs = [Document(content=text, metadata={"source": "test.md"})]

    # ë°©ë²• 1: ê°€ì¥ ê°„ë‹¨ (ìë™)
    print("\n1. ê°€ì¥ ê°„ë‹¨ (ìë™ ìµœì í™”):")
    chunks = TextSplitter.split(docs, chunk_size=100)
    print(f"   âœ“ ìë™: {len(chunks)} ì²­í¬")

    # ë°©ë²• 2: ì „ëµ íŒ©í† ë¦¬ ë©”ì„œë“œ (ì¶”ì²œ!)
    print("\n2. ì „ëµ íŒ©í† ë¦¬ ë©”ì„œë“œ (ì‰½ê³  ëª…í™•!):")

    # Recursive (ê¶Œì¥)
    splitter_rec = TextSplitter.recursive(chunk_size=100)
    chunks_rec = splitter_rec.split_documents(docs)
    print(f"   âœ“ TextSplitter.recursive(): {len(chunks_rec)} ì²­í¬")

    # Character (ë‹¨ìˆœ)
    splitter_char = TextSplitter.character(separator="\n\n")
    chunks_char = splitter_char.split_documents(docs)
    print(f"   âœ“ TextSplitter.character(): {len(chunks_char)} ì²­í¬")

    # Markdown (í—¤ë” ê¸°ì¤€)
    splitter_md = TextSplitter.markdown()
    chunks_md = splitter_md.split_documents(docs)
    print(f"   âœ“ TextSplitter.markdown(): {len(chunks_md)} ì²­í¬")
    print(f"      ì²« ë²ˆì§¸ ì²­í¬ ë©”íƒ€ë°ì´í„°: {chunks_md[0].metadata}")

    # ë°©ë²• 3: êµ¬ë¶„ìë§Œ ì§€ì • (ìë™ ì „ëµ ì„ íƒ)
    print("\n3. êµ¬ë¶„ìë§Œ ì§€ì • (í¸ë¦¬!):")
    chunks_sep = TextSplitter.split(docs, separator="\n\n")
    print(f"   âœ“ separator='\\n\\n': {len(chunks_sep)} ì²­í¬")

    chunks_seps = TextSplitter.split(docs, separators=["##", "\n\n"])
    print(f"   âœ“ separators=['##', '\\n\\n']: {len(chunks_seps)} ì²­í¬")

    # ë°©ë²• 4: ì „ëµ ë¬¸ìì—´ ì§€ì • (ê¸°ì¡´ ë°©ì‹)
    print("\n4. ì „ëµ ë¬¸ìì—´ ì§€ì • (ê¸°ì¡´ ë°©ì‹):")
    chunks_str = TextSplitter.split(docs, strategy="recursive", chunk_size=100)
    print(f"   âœ“ strategy='recursive': {len(chunks_str)} ì²­í¬")

    print("\nâœ“ TextSplitter: 4ê°€ì§€ ë°©ë²• ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥!")


def demo_advanced_customization():
    """ê³ ê¸‰ ì»¤ìŠ¤í„°ë§ˆì´ì§• ì˜ˆì œ"""
    print("\n" + "="*60)
    print("ğŸ”§ ê³ ê¸‰ ì»¤ìŠ¤í„°ë§ˆì´ì§• ë°ëª¨")
    print("="*60)

    text = "AI is amazing. " * 50
    docs = [Document(content=text, metadata={"source": "test"})]

    # 1. Recursive with custom separators
    print("\n1. Recursive + ì»¤ìŠ¤í…€ êµ¬ë¶„ì:")
    splitter = TextSplitter.recursive(
        chunk_size=100,
        chunk_overlap=20,
        separators=[". ", " "]  # ë¬¸ì¥ ìš°ì„ , ê·¸ ë‹¤ìŒ ë‹¨ì–´
    )
    chunks = splitter.split_documents(docs)
    print(f"   âœ“ {len(chunks)} ì²­í¬ ìƒì„±")
    print(f"   âœ“ ì²« ë²ˆì§¸ ì²­í¬: {chunks[0].content[:50]}...")

    # 2. Character with custom separator
    print("\n2. Character + ì»¤ìŠ¤í…€ êµ¬ë¶„ì:")
    splitter = TextSplitter.character(
        separator=". ",
        chunk_size=80,
        chunk_overlap=10
    )
    chunks = splitter.split_documents(docs)
    print(f"   âœ“ {len(chunks)} ì²­í¬ ìƒì„±")

    # 3. Markdown with custom headers
    print("\n3. Markdown + ì»¤ìŠ¤í…€ í—¤ë”:")
    md_text = """
# Title
Content 1

## Section
Content 2

### Subsection
Content 3
    """.strip()

    md_docs = [Document(content=md_text, metadata={})]

    splitter = TextSplitter.markdown(
        headers_to_split_on=[
            ("#", "Title"),
            ("##", "Section"),
            ("###", "Subsection"),
        ]
    )
    chunks = splitter.split_documents(md_docs)
    print(f"   âœ“ {len(chunks)} ì²­í¬ (í—¤ë” ê¸°ì¤€)")
    for i, chunk in enumerate(chunks):
        print(f"      Chunk {i+1}: {chunk.metadata}")

    print("\nâœ“ ê³ ê¸‰ ì»¤ìŠ¤í„°ë§ˆì´ì§•: ì„¸ë°€í•œ ì œì–´ ê°€ëŠ¥!")


def demo_real_world_usage():
    """ì‹¤ì „ ì‚¬ìš© ì˜ˆì œ"""
    print("\n" + "="*60)
    print("ğŸš€ ì‹¤ì „ ì‚¬ìš© ì˜ˆì œ")
    print("="*60)

    # ì‹œë‚˜ë¦¬ì˜¤ 1: PDF ë¬¸ì„œë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• 
    print("\nì‹œë‚˜ë¦¬ì˜¤ 1: ë¬¸ì„œ ë¡œë”© â†’ ë¶„í•  (ê°„ë‹¨!)")

    test_file = Path("document.txt")
    test_file.write_text("""
Introduction to AI

Artificial Intelligence (AI) is revolutionizing technology.
Machine learning is a subset of AI.

Deep Learning

Deep learning uses neural networks with multiple layers.
It powers modern AI applications.

Applications

AI is used in various fields: healthcare, finance, and more.
    """.strip(), encoding="utf-8")

    try:
        # í•œ ì¤„ì”© ê°„ë‹¨í•˜ê²Œ!
        docs = DocumentLoader.load(test_file)

        # ì „ëµì„ ì‰½ê²Œ ì„ íƒ
        chunks = TextSplitter.recursive(chunk_size=100).split_documents(docs)

        print(f"   âœ“ {len(docs)} ë¬¸ì„œ â†’ {len(chunks)} ì²­í¬")
        print(f"   âœ“ ì²« ë²ˆì§¸ ì²­í¬: {chunks[0].content[:50]}...")

    finally:
        test_file.unlink()

    # ì‹œë‚˜ë¦¬ì˜¤ 2: íŠ¹ì • êµ¬ë¶„ìë¡œ ë¶„í• 
    print("\nì‹œë‚˜ë¦¬ì˜¤ 2: íŠ¹ì • êµ¬ë¶„ìë¡œ ë¶„í•  (í¸ë¦¬!)")

    log_text = """
[INFO] 2024-01-01: System started
[INFO] 2024-01-01: Processing data
---
[ERROR] 2024-01-02: Connection failed
[INFO] 2024-01-02: Retrying...
---
[INFO] 2024-01-03: Success
    """.strip()

    log_docs = [Document(content=log_text, metadata={"source": "log.txt"})]

    # "---"ë¡œ ë¶„í•  (ê°„ë‹¨!)
    chunks = TextSplitter.character(separator="---").split_documents(log_docs)
    print(f"   âœ“ '---' êµ¬ë¶„ìë¡œ {len(chunks)} ì²­í¬")
    for i, chunk in enumerate(chunks[:2]):
        print(f"      Chunk {i+1}: {chunk.content.strip()[:50]}...")

    # ì‹œë‚˜ë¦¬ì˜¤ 3: ì—¬ëŸ¬ êµ¬ë¶„ìë¡œ ê³„ì¸µì  ë¶„í• 
    print("\nì‹œë‚˜ë¦¬ì˜¤ 3: ê³„ì¸µì  ë¶„í•  (ë˜‘ë˜‘!)")

    # separators íŒŒë¼ë¯¸í„°ë¡œ ê°„ë‹¨íˆ!
    chunks = TextSplitter.split(
        log_docs,
        separators=["---", "\n", " "],
        chunk_size=80
    )
    print(f"   âœ“ ê³„ì¸µì  êµ¬ë¶„ìë¡œ {len(chunks)} ì²­í¬")

    print("\nâœ“ ì‹¤ì „ ì‚¬ìš©: ê°„ë‹¨í•˜ê³  ì§ê´€ì !")


def demo_comparison():
    """LangChain vs beanllm ë¹„êµ"""
    print("\n" + "="*60)
    print("ğŸ“Š LangChain vs beanllm ë¹„êµ")
    print("="*60)

    print("\nã€ LangChain ë°©ì‹ ã€‘(ë³µì¡)")
    print("""
    # 1. Import ì—¬ëŸ¬ ê°œ
    from langchain.document_loaders import TextLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter

    # 2. Loader ìˆ˜ë™ ì„ íƒ
    loader = TextLoader("file.txt")
    docs = loader.load()

    # 3. Splitter ìˆ˜ë™ ì„¤ì •
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
        separators=["\\n\\n", "\\n", ". ", " ", ""]
    )
    chunks = splitter.split_documents(docs)
    """)

    print("\nã€ beanllm ë°©ì‹ ã€‘(ê°„ë‹¨!)")
    print("""
    # 1. Import í•œ ë²ˆ
    from beanllm import DocumentLoader, TextSplitter

    # 2. ìë™ ê°ì§€ ë¡œë”©
    docs = DocumentLoader.load("file.txt")

    # 3. ì „ëµ ì‰½ê²Œ ì„ íƒ
    chunks = TextSplitter.recursive().split_documents(docs)

    # ë˜ëŠ” ë” ê°„ë‹¨í•˜ê²Œ
    chunks = TextSplitter.split(docs)
    """)

    print("\nâœ… beanllm: ~10ì¤„ â†’ 2-3ì¤„ (70% ê°ì†Œ!)")
    print("âœ… ìë™ ê°ì§€ + ìŠ¤ë§ˆíŠ¸ ê¸°ë³¸ê°’ + ì‰¬ìš´ ì»¤ìŠ¤í„°ë§ˆì´ì§•")


def main():
    """ëª¨ë“  ë°ëª¨ ì‹¤í–‰"""
    print("="*60)
    print("ğŸ¯ ê°œì„ ëœ API ë°ëª¨")
    print("="*60)
    print("\nbeanllmì˜ ì² í•™:")
    print("  1. ìë™ ê°ì§€ (80% ì¼€ì´ìŠ¤)")
    print("  2. ëª…ì‹œì  ì„ íƒ (ì„¸ë°€í•œ ì œì–´)")
    print("  3. ë‘˜ ë‹¤ ê°€ëŠ¥!")

    demo_loader_type_selection()
    demo_splitter_strategies()
    demo_advanced_customization()
    demo_real_world_usage()
    demo_comparison()

    print("\n" + "="*60)
    print("ğŸ‰ ê°œì„  ì™„ë£Œ!")
    print("="*60)
    print("\nâœ¨ ì£¼ìš” ê°œì„ ì‚¬í•­:")
    print("  1. DocumentLoader.load(file, loader_type='pdf')")
    print("  2. TextSplitter.recursive(chunk_size=1000)")
    print("  3. TextSplitter.character(separator='\\n\\n')")
    print("  4. TextSplitter.split(docs, separator='---')")
    print("  5. TextSplitter.split(docs, separators=['\\n\\n', '\\n'])")
    print("\nğŸ’¡ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ëŒ€ë¡œ ì‰½ê²Œ ì„¤ì •í•˜ê³  ì¡°ì • ê°€ëŠ¥!")


if __name__ == "__main__":
    main()
