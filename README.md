# llmkit

**Unified toolkit for managing and using multiple LLM providers**

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ¯ Features

- **ğŸ”„ Unified Interface**: Use OpenAI, Claude, Gemini, and Ollama with the same API
- **ğŸ“Š Model Registry**: Auto-detect available models from your API keys
- **ğŸ›ï¸ Parameter Adaptation**: Automatically convert parameters for each provider
  - `max_tokens` â†’ `max_completion_tokens` (OpenAI GPT-5)
  - `max_tokens` â†’ `max_output_tokens` (Gemini)
  - `max_tokens` â†’ `num_predict` (Ollama)
- **ğŸ“¦ Zero External Dependencies**: No `src.*` imports, fully independent
- **ğŸ” CLI Tools**: Inspect models and capabilities from command line
- **ğŸš€ Pattern-Based Inference**: Auto-detect new model capabilities

---

## ğŸ“¦ Installation

### Quick Start (Recommended) â­

```bash
pip install llmkit
```

**What's included by default:**
- âœ… Model registry and CLI tools
- âœ… **OpenAI** SDK (GPT-4, GPT-5, etc.)
- âœ… **Anthropic** SDK (Claude 3.5, etc.)

**Optional providers:**
- Gemini and Ollama are optional (see below)

This covers the most commonly used providers out of the box!

**After installation, see the welcome message:**
```bash
python -m llmkit.scripts.welcome
# or
python scripts/welcome.py
```

---

### Install Additional Providers

```bash
# Add Gemini support
pip install llmkit[gemini]

# Add Ollama support (local models)
pip install llmkit[ollama]

# Install all providers (Gemini + Ollama)
pip install llmkit[all]
```

---

### Installation Guide

| Command | OpenAI | Claude | Gemini | Ollama |
|---------|--------|--------|--------|--------|
| `pip install llmkit` | âœ… | âœ… | âŒ | âŒ |
| `pip install llmkit[gemini]` | âœ… | âœ… | âœ… | âŒ |
| `pip install llmkit[ollama]` | âœ… | âœ… | âŒ | âœ… |
| `pip install llmkit[all]` | âœ… | âœ… | âœ… | âœ… |

ğŸ’¡ **Tip:** If you try to use a provider without its SDK, llmkit will show you a helpful install message!

### Development Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/llmkit.git
cd llmkit

# Install in editable mode with dev dependencies
pip install -e ".[dev,all]"
```

---

## ğŸš€ Quick Start

### 1. Set up environment variables

```bash
# .env file or export
export OPENAI_API_KEY="your-key"
export ANTHROPIC_API_KEY="your-key"
export GEMINI_API_KEY="your-key"
export OLLAMA_HOST="http://localhost:11434"
```

### 2. Python Usage

```python
from llmkit import get_registry

# Get model registry
registry = get_registry()

# Check active providers
active_providers = registry.get_active_providers()
print(f"Active: {[p.name for p in active_providers]}")
# â†’ Active: ['openai', 'ollama']

# Get available models
models = registry.get_available_models()
for model in models:
    print(f"{model.model_name} ({model.provider})")
# â†’ gpt-4o (openai)
# â†’ gpt-4o-mini (openai)
# â†’ claude-3-5-sonnet-20241022 (anthropic)
# â†’ ...

# Get model info
model_info = registry.get_model_info("gpt-4o-mini")
print(f"Streaming: {model_info.supports_streaming}")
print(f"Temperature: {model_info.supports_temperature}")
print(f"Max Tokens: {model_info.supports_max_tokens}")
```

### 3. CLI Usage

```bash
# List all available models
llmkit list

# Show specific model details
llmkit show gpt-4o-mini

# List active providers
llmkit providers

# Show summary
llmkit summary

# Export all model info as JSON
llmkit export > models.json
```

---

## ğŸ“š Detailed Usage

### Model Registry

```python
from llmkit import get_registry

registry = get_registry()

# Get models by provider
openai_models = registry.get_available_models(provider="openai")
claude_models = registry.get_available_models(provider="anthropic")

# Get specific model info
model = registry.get_model_info("gpt-4o")
if model:
    print(f"Display Name: {model.display_name}")
    print(f"Max Tokens: {model.max_tokens}")
    print(f"Temperature Range: {model.default_temperature}")

    # Check parameter support
    for param in model.parameters:
        status = "âœ…" if param.supported else "âŒ"
        print(f"{status} {param.name}: {param.description}")
```

### Provider Information

```python
from llmkit import get_registry

registry = get_registry()

# Get all providers
providers = registry.get_all_providers()

for name, provider in providers.items():
    print(f"Provider: {name}")
    print(f"  Status: {provider.status.value}")
    print(f"  Env Key: {provider.env_key}")
    print(f"  Available: {provider.env_value_set}")
    print(f"  Models: {len(provider.available_models)}")
    print(f"  Default: {provider.default_model}")
```

### Using with Actual LLM Calls

```python
# Example with OpenAI (if you have openai installed)
from llmkit import get_registry

registry = get_registry()
model_info = registry.get_model_info("gpt-4o-mini")

# Get parameter configuration
params = {}
if model_info.supports_temperature:
    params["temperature"] = 0.7
if model_info.uses_max_completion_tokens:
    params["max_completion_tokens"] = 1000
elif model_info.supports_max_tokens:
    params["max_tokens"] = 1000

print(f"Using parameters: {params}")
# â†’ Using parameters: {'temperature': 0.7, 'max_tokens': 1000}
```

---

## ğŸ” CLI Commands

### `llmkit list`

List all available models with their capabilities.

```bash
$ llmkit list

í™œì„±í™”ëœ ì œê³µì: openai, ollama
ì´ ëª¨ë¸ ìˆ˜: 25

âœ… gpt-4o (openai)
   Streaming: True
   Temperature: True
   Max Tokens: True

âœ… gpt-5-nano (openai)
   Streaming: True
   Temperature: False
   Max Tokens: False
   Uses max_completion_tokens: True

âœ… phi3.5 (ollama)
   Streaming: True
   Temperature: True
   Max Tokens: True
```

### `llmkit show <model>`

Show detailed information about a specific model.

```bash
$ llmkit show gpt-4o-mini

ëª¨ë¸: gpt-4o-mini
ì œê³µì: openai
ì„¤ëª…: OpenAIì˜ ë¹ ë¥´ê³  ì €ë ´í•œ ëª¨ë¸

ê¸°ëŠ¥:
  - Streaming: âœ… Yes
  - Temperature: âœ… Yes (0.0-2.0)
  - Max Tokens: âœ… Yes (16384)

íŒŒë¼ë¯¸í„°:
  âœ… temperature (float)
     ê¸°ë³¸ê°’: 0.0
     í•„ìˆ˜: False
     ì„¤ëª…: ì‘ë‹µì˜ ì°½ì˜ì„±/ëœë¤ì„± ì¡°ì ˆ

  âœ… max_tokens (int)
     ê¸°ë³¸ê°’: 16384
     í•„ìˆ˜: False
     ì„¤ëª…: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜

ì‚¬ìš© ì˜ˆì œ:
[... ì½”ë“œ ì˜ˆì‹œ ...]
```

### `llmkit providers`

Show all configured providers.

```bash
$ llmkit providers

ì œê³µì ëª©ë¡:

âœ… openai
   ìƒíƒœ: active
   í™˜ê²½ë³€ìˆ˜: OPENAI_API_KEY = ì„¤ì •ë¨
   ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: 15
   ê¸°ë³¸ ëª¨ë¸: gpt-4o-mini

âŒ anthropic
   ìƒíƒœ: inactive
   í™˜ê²½ë³€ìˆ˜: ANTHROPIC_API_KEY = ë¯¸ì„¤ì •
   ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: 0

âœ… ollama
   ìƒíƒœ: active
   í™˜ê²½ë³€ìˆ˜: OLLAMA_HOST = ì„¤ì •ë¨
   ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: 4
   ê¸°ë³¸ ëª¨ë¸: qwen2.5:7b
```

### `llmkit summary`

Show quick summary.

```bash
$ llmkit summary

ìš”ì•½ ì •ë³´:

ì´ ì œê³µì: 4
í™œì„±í™”ëœ ì œê³µì: 2
ì´ ëª¨ë¸ ìˆ˜: 19

í™œì„±í™”ëœ ì œê³µì: openai, ollama
```

---

## ğŸ¨ Model Information Structure

Each model provides detailed capability information:

```python
@dataclass
class ModelCapabilityInfo:
    model_name: str                    # "gpt-4o-mini"
    display_name: str                  # "GPT-4o Mini"
    provider: str                      # "openai"
    model_type: str                    # "llm"

    supports_streaming: bool           # True
    supports_temperature: bool         # True
    supports_max_tokens: bool          # True
    uses_max_completion_tokens: bool   # False (True for GPT-5)

    max_tokens: int                    # 16384
    default_temperature: float         # 0.0

    description: str                   # Model description
    use_case: str                      # Recommended use case
    parameters: List[ParameterInfo]    # Detailed parameter info
    example_usage: str                 # Code examples
```

---

## ğŸ”§ Configuration

### Environment Variables

```bash
# OpenAI
OPENAI_API_KEY="sk-..."

# Anthropic Claude
ANTHROPIC_API_KEY="sk-ant-..."

# Google Gemini
GEMINI_API_KEY="..."

# Ollama (local)
OLLAMA_HOST="http://localhost:11434"
```

### Using .env File

```bash
# Create .env file in your project root
cat > .env << EOF
OPENAI_API_KEY=your-key
ANTHROPIC_API_KEY=your-key
GEMINI_API_KEY=your-key
OLLAMA_HOST=http://localhost:11434
EOF
```

llmkit will automatically load from `.env` if `python-dotenv` is installed.

---

## ğŸ“– Model Support

### OpenAI

- âœ… GPT-4o, GPT-4o-mini, GPT-4-turbo
- âœ… GPT-5, GPT-5-mini, GPT-5-nano (with `max_completion_tokens`)
- âœ… GPT-4.1 series (with `max_completion_tokens`)
- âœ… O3, O3-mini, O4-mini (reasoning models)
- âœ… Auto-detection of new models
- âœ… Date-versioned models (e.g., `gpt-5-nano-2025-08-07`)

### Anthropic Claude

- âœ… Claude 3.5 Sonnet, Claude 3 Opus, Claude 3 Haiku
- âœ… Temperature range: 0.0-1.0 (auto-clamped)
- âœ… Date-versioned models (e.g., `claude-3-5-sonnet-20241022`)

### Google Gemini

- âœ… Gemini 2.5 Flash, Gemini 2.5 Pro
- âœ… Gemini 2.0, Gemini 1.5 series
- âœ… `max_output_tokens` parameter
- âœ… Thinking mode support (2.5+)

### Ollama

- âœ… All local models
- âœ… `num_predict` parameter
- âœ… Dynamic model detection

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=llmkit --cov-report=html

# Run specific test
pytest tests/test_registry.py

# Run async tests
pytest tests/test_providers.py -v
```

---

## ğŸ› ï¸ Development

```bash
# Install in editable mode
pip install -e ".[dev,all]"

# Format code
black llmkit tests

# Lint
ruff check llmkit

# Type check
mypy llmkit
```

---

## ğŸ“ Examples

See [examples/](examples/) directory for more examples:

- [`basic_usage.py`](examples/basic_usage.py) - Basic registry usage
- [`check_providers.py`](examples/check_providers.py) - Check active providers
- [`model_params.py`](examples/model_params.py) - Get model parameters
- [`test_import.py`](examples/test_import.py) - Test package import

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

llmkitì€ ë‹¤ìŒ í”„ë¡œì íŠ¸ë“¤ì—ì„œ ì˜ê°ì„ ë°›ì•˜ìŠµë‹ˆë‹¤:

### ì°¸ê³ í•œ í”„ë¡œì íŠ¸

- **[LangChain](https://github.com/langchain-ai/langchain)**: LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬ì˜ ì„ êµ¬ì. ì²´ì¸, ì—ì´ì „íŠ¸, ë©”ëª¨ë¦¬ ë“±ì˜ ê°œë…ì„ ì°¸ê³ í–ˆìŠµë‹ˆë‹¤.
- **[Claude (Anthropic)](https://www.anthropic.com/)**: ëª…í™•í•˜ê³  ê°„ê²°í•œ ì½”ë“œ ì‘ì„± ì² í•™ì˜ ì˜ê°ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. ëª¨í†  "Claude Code"ëŠ” ì—¬ê¸°ì„œ ìœ ë˜í–ˆìŠµë‹ˆë‹¤.
- **[TeddyNote](https://github.com/teddynote/teddynote)**: í„°ë¯¸ë„ UI ë””ìì¸ê³¼ ì‚¬ìš©ì ê²½í—˜ì— ëŒ€í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí–ˆìŠµë‹ˆë‹¤.

### ë¼ì´ì„¼ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„¼ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ì°¸ê³ í•œ í”„ë¡œì íŠ¸ë“¤ì˜ ë¼ì´ì„¼ìŠ¤:
- LangChain: MIT License
- Claude API: Anthropicì˜ API ì„œë¹„ìŠ¤ ì•½ê´€
- TeddyNote: í•´ë‹¹ í”„ë¡œì íŠ¸ì˜ ë¼ì´ì„¼ìŠ¤ ì •ì±…

### ê°ì‚¬ì˜ ë§

- OpenAI for GPT models
- Anthropic for Claude API
- Google for Gemini API
- Ollama team for local LLM support
- Rich library for beautiful terminal UI

---

## ğŸ“§ Contact

- Issues: https://github.com/yourusername/llmkit/issues
- Discussions: https://github.com/yourusername/llmkit/discussions

---

## ğŸ—ºï¸ Roadmap

- [ ] Automatic model metadata updates (LLM-assisted)
- [ ] Unified LLM interface (single API for all providers)
- [ ] Parameter adapter (auto-convert parameters)
- [ ] Model performance benchmarks
- [ ] Integration with LangChain
- [ ] Web dashboard

---

**Made with â¤ï¸ for the LLM community**
