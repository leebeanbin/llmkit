# ğŸš€ llmkit ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

## ğŸ“¦ ì„¤ì¹˜

### Poetry ì‚¬ìš© (ê¶Œì¥)

```bash
# í”„ë¡œì íŠ¸ í´ë¡ 
git clone https://github.com/yourusername/llmkit.git
cd llmkit

# Poetry ì„¤ì¹˜ (ì—†ëŠ” ê²½ìš°)
curl -sSL https://install.python-poetry.org | python3 -

# ì˜ì¡´ì„± ì„¤ì¹˜
poetry install --extras all  # ëª¨ë“  Provider í¬í•¨
# ë˜ëŠ”
poetry install --extras openai  # OpenAIë§Œ

# ê°€ìƒ í™˜ê²½ í™œì„±í™”
poetry shell
```

### pip ì‚¬ìš©

```bash
# ê¸°ë³¸ ì„¤ì¹˜
pip install llmkit

# íŠ¹ì • Provider ì¶”ê°€
pip install llmkit[openai]
pip install llmkit[anthropic]
pip install llmkit[gemini]
pip install llmkit[ollama]

# ëª¨ë“  Provider
pip install llmkit[all]

# ê°œë°œ ë„êµ¬ í¬í•¨
pip install llmkit[dev,all]
```

---

## âš™ï¸ í™˜ê²½ ì„¤ì •

### 1. .env íŒŒì¼ ìƒì„±

```bash
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— .env íŒŒì¼ ìƒì„±
touch .env
```

### 2. API í‚¤ ì„¤ì •

```env
# OpenAI
OPENAI_API_KEY=sk-...

# Anthropic Claude
ANTHROPIC_API_KEY=sk-ant-...

# Google Gemini
GEMINI_API_KEY=...

# Ollama (ë¡œì»¬, API í‚¤ ë¶ˆí•„ìš”)
OLLAMA_HOST=http://localhost:11434
```

### 3. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ

```python
# ìë™ìœ¼ë¡œ .env íŒŒì¼ ë¡œë“œë¨
from llmkit import Client
# ë˜ëŠ”
from dotenv import load_dotenv
load_dotenv()
```

---

## ğŸ¯ ê¸°ë³¸ ì‚¬ìš©ë²•

### 1. ê°„ë‹¨í•œ ì±„íŒ…

```python
from llmkit import Client

# Client ìƒì„± (ìë™ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ Provider ì„ íƒ)
client = Client(model="gpt-4o")

# ì±„íŒ…
response = client.chat("ì•ˆë…•í•˜ì„¸ìš”!")
print(response.content)

# ìŠ¤íŠ¸ë¦¬ë°
for chunk in client.stream("ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì„¸ìš”"):
    print(chunk.content, end="", flush=True)
```

### 2. Provider ì„ íƒ

```python
# OpenAI ì‚¬ìš©
client = Client(model="gpt-4o")

# Claude ì‚¬ìš©
client = Client(model="claude-3-5-sonnet-20241022")

# Gemini ì‚¬ìš©
client = Client(model="gemini-2.0-flash-exp")

# Ollama ì‚¬ìš© (ë¡œì»¬)
client = Client(model="qwen2.5:7b")
```

### 3. íŒŒë¼ë¯¸í„° ì„¤ì •

```python
response = client.chat(
    "ì°½ì˜ì ì¸ ì´ì•¼ê¸°ë¥¼ ì¨ì£¼ì„¸ìš”",
    temperature=0.9,      # ì°½ì˜ì„±
    max_tokens=1000,      # ìµœëŒ€ í† í°
    system="ë‹¹ì‹ ì€ ì°½ì˜ì ì¸ ì‘ê°€ì…ë‹ˆë‹¤"  # ì‹œìŠ¤í…œ ë©”ì‹œì§€
)
```

---

## ğŸ“„ RAG (Retrieval-Augmented Generation)

### 1. ë¬¸ì„œì—ì„œ RAG ìƒì„±

```python
from llmkit import RAGChain

# ë¬¸ì„œ í´ë”ì—ì„œ RAG ìƒì„±
rag = RAGChain.from_documents("docs/")

# ì§ˆë¬¸í•˜ê¸°
answer = rag.query("ì´ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì€?")
print(answer)

# ì†ŒìŠ¤ í¬í•¨
answer, sources = rag.query(
    "êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    include_sources=True
)

for source in sources:
    print(f"ì¶œì²˜: {source.document.metadata.get('source')}")
    print(f"ìœ ì‚¬ë„: {source.similarity:.4f}")
```

### 2. ì»¤ìŠ¤í…€ RAG êµ¬ì„±

```python
from llmkit import (
    DocumentLoader,
    RecursiveCharacterTextSplitter,
    OpenAIEmbedding,
    ChromaVectorStore,
    RAGChain
)

# 1. ë¬¸ì„œ ë¡œë“œ
docs = DocumentLoader.load("my_documents/")

# 2. í…ìŠ¤íŠ¸ ë¶„í• 
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
chunks = splitter.split_documents(docs)

# 3. ì„ë² ë”© ìƒì„±
embedding = OpenAIEmbedding(model="text-embedding-3-small")

# 4. ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
vector_store = ChromaVectorStore.from_documents(
    documents=chunks,
    embedding=embedding,
    persist_directory="./my_vector_db"
)

# 5. RAG ìƒì„±
rag = RAGChain(
    vector_store=vector_store,
    llm=Client(model="gpt-4o")
)

# ì‚¬ìš©
answer = rag.query("ì§ˆë¬¸")
```

---

## ğŸ¤– Agent (ë„êµ¬ ì‚¬ìš©)

### 1. ê¸°ë³¸ Agent

```python
from llmkit import Agent, Tool

# ë„êµ¬ ì •ì˜
@Tool.from_function
def calculator(expression: str) -> str:
    """ìˆ˜í•™ í‘œí˜„ì‹ì„ ê³„ì‚°í•©ë‹ˆë‹¤"""
    return str(eval(expression))

@Tool.from_function
def get_weather(city: str) -> str:
    """ë„ì‹œì˜ ë‚ ì”¨ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤"""
    # ì‹¤ì œ API í˜¸ì¶œ
    return f"{city}ì˜ ë‚ ì”¨ëŠ” ë§‘ìŒì…ë‹ˆë‹¤"

# Agent ìƒì„±
agent = Agent(
    llm=Client(model="gpt-4o"),
    tools=[calculator, get_weather],
    max_iterations=10
)

# ì‹¤í–‰
result = agent.run("25 * 17ë¥¼ ê³„ì‚°í•˜ê³ , ì„œìš¸ì˜ ë‚ ì”¨ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”")
print(result.output)
```

### 2. ë‚´ì¥ ë„êµ¬ ì‚¬ìš©

```python
from llmkit import Agent, search_web, get_current_time

# ë‚´ì¥ ë„êµ¬ ì‚¬ìš©
agent = Agent(
    llm=Client(model="gpt-4o"),
    tools=[search_web, get_current_time]
)

result = agent.run("í˜„ì¬ ì‹œê°„ì„ ì•Œë ¤ì£¼ê³ , ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•´ì£¼ì„¸ìš”")
```

---

## ğŸ•¸ï¸ Graph Workflows

### 1. ê°„ë‹¨í•œ Graph

```python
from llmkit import StateGraph, END

# Graph ìƒì„±
graph = StateGraph()

# ë…¸ë“œ ì •ì˜
def analyze(state):
    state["analysis"] = client.chat(f"ë¶„ì„: {state['input']}")
    return state

def improve(state):
    state["output"] = client.chat(f"ê°œì„ : {state['input']}")
    return state

# ë…¸ë“œ ì¶”ê°€
graph.add_node("analyze", analyze)
graph.add_node("improve", improve)

# ì¡°ê±´ë¶€ ì—£ì§€
def should_improve(state):
    score = float(state["analysis"].content.split("ì ìˆ˜:")[1])
    return "improve" if score < 0.8 else "end"

graph.add_conditional_edges(
    "analyze",
    should_improve,
    {"improve": "improve", "end": END}
)

# ì‹¤í–‰
result = graph.compile().invoke({"input": "ì´ˆì•ˆ í…ìŠ¤íŠ¸"})
print(result["output"])
```

### 2. LangGraph ìŠ¤íƒ€ì¼

```python
from llmkit import Graph, create_simple_graph

# ê°„ë‹¨í•œ Graph ìƒì„±
graph = create_simple_graph(
    nodes={
        "research": lambda s: {"info": "ì—°êµ¬ ê²°ê³¼"},
        "write": lambda s: {"draft": "ì´ˆì•ˆ"},
        "review": lambda s: {"final": "ìµœì¢…"}
    },
    edges=[
        ("research", "write"),
        ("write", "review")
    ]
)

result = graph.run({"topic": "AI"})
```

---

## ğŸ‘¥ Multi-Agent Systems

### 1. Debate íŒ¨í„´

```python
from llmkit import MultiAgentCoordinator, DebateStrategy, Agent

# ì—¬ëŸ¬ Agent ìƒì„±
researcher = Agent(
    llm=Client(model="gpt-4o"),
    role="ì—°êµ¬ì",
    tools=[search_web]
)

writer = Agent(
    llm=Client(model="gpt-4o"),
    role="ì‘ê°€"
)

critic = Agent(
    llm=Client(model="gpt-4o"),
    role="ë¹„í‰ê°€"
)

# Coordinator ìƒì„±
coordinator = MultiAgentCoordinator(
    agents=[researcher, writer, critic],
    strategy=DebateStrategy(rounds=3)
)

# ì‹¤í–‰
result = coordinator.coordinate("ì–‘ì ì»´í“¨íŒ…ì— ëŒ€í•œ ê¸°ì‚¬ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”")
print(result.final_output)
```

### 2. Sequential íŒ¨í„´

```python
from llmkit import SequentialStrategy

coordinator = MultiAgentCoordinator(
    agents=[researcher, writer, critic],
    strategy=SequentialStrategy()
)

result = coordinator.coordinate("ì‘ì—…ì„ ìˆœì°¨ì ìœ¼ë¡œ ìˆ˜í–‰")
```

---

## ğŸ–¼ï¸ Vision RAG

### 1. ì´ë¯¸ì§€ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ

```python
from llmkit import VisionRAG, CLIPEmbedding, ImageLoader

# ì´ë¯¸ì§€ ë¡œë“œ
images = ImageLoader.load("images/")

# Vision RAG ìƒì„±
vision_rag = VisionRAG.from_images(
    images=images,
    embedding=CLIPEmbedding(),
    llm=Client(model="gpt-4o")  # Vision ì§€ì› ëª¨ë¸
)

# í…ìŠ¤íŠ¸ ì§ˆì˜
answer = vision_rag.query("ì´ ì´ë¯¸ì§€ë“¤ì— ì–´ë–¤ ê°ì²´ë“¤ì´ ìˆë‚˜ìš”?")

# ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ì§ˆì˜
answer = vision_rag.query_with_image(
    "reference.jpg",
    "ì´ ì´ë¯¸ì§€ì™€ ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ì°¾ì•„ ì„¤ëª…í•´ì£¼ì„¸ìš”"
)
```

---

## ğŸ™ï¸ Audio Processing

### 1. Speech-to-Text

```python
from llmkit import WhisperSTT

stt = WhisperSTT()
result = stt.transcribe("audio.mp3", language="ko")
print(result.text)

# ì„¸ê·¸ë¨¼íŠ¸ë³„ ê²°ê³¼
for segment in result.segments:
    print(f"{segment.start:.2f}s - {segment.end:.2f}s: {segment.text}")
```

### 2. Text-to-Speech

```python
from llmkit import TextToSpeech

tts = TextToSpeech(provider="openai")
audio = tts.synthesize(
    "ì•ˆë…•í•˜ì„¸ìš”, ë°˜ê°‘ìŠµë‹ˆë‹¤",
    voice="alloy",
    speed=1.0
)

# íŒŒì¼ ì €ì¥
audio.save("output.mp3")
```

### 3. Audio RAG

```python
from llmkit import AudioRAG

# ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ RAG ìƒì„±
audio_rag = AudioRAG.from_audio_files([
    "podcast1.mp3",
    "podcast2.mp3"
])

# ì§ˆë¬¸
answer = audio_rag.query("AIì— ëŒ€í•´ ë¬´ì—‡ì´ ë…¼ì˜ë˜ì—ˆë‚˜ìš”?")
```

---

## ğŸŒ Web Search

### 1. ì›¹ ê²€ìƒ‰

```python
from llmkit import DuckDuckGoSearch, WebScraper

# ê²€ìƒ‰ (API í‚¤ ë¶ˆí•„ìš”!)
search = DuckDuckGoSearch()
results = search.search("ìµœì‹  AI ë‰´ìŠ¤", max_results=5)

for result in results:
    print(f"{result.title}: {result.url}")
    print(f"ìš”ì•½: {result.snippet}")

# ì½˜í…ì¸  ìŠ¤í¬ë˜í•‘
scraper = WebScraper()
content = scraper.scrape(results[0].url)
print(content)
```

---

## ğŸ“Š Evaluation

### 1. í…ìŠ¤íŠ¸ í‰ê°€

```python
from llmkit import evaluate_text

prediction = "ê³ ì–‘ì´ê°€ ë§¤íŠ¸ ìœ„ì— ì•‰ì•„ìˆë‹¤"
reference = "ê³ ì–‘ì´ê°€ ë§¤íŠ¸ ìœ„ì— ì•‰ì•„ ìˆìŠµë‹ˆë‹¤"

result = evaluate_text(
    prediction=prediction,
    reference=reference,
    metrics=["bleu", "rouge-1", "rouge-l", "f1"]
)

print(f"BLEU: {result.bleu:.4f}")
print(f"ROUGE-1: {result.rouge_1:.4f}")
print(f"í‰ê·  ì ìˆ˜: {result.average_score:.4f}")
```

### 2. RAG í‰ê°€

```python
from llmkit import evaluate_rag

rag_result = evaluate_rag(
    question="AIë€ ë¬´ì—‡ì¸ê°€ìš”?",
    answer="AIëŠ” ì¸ê³µì§€ëŠ¥ì…ë‹ˆë‹¤...",
    contexts=["ì»¨í…ìŠ¤íŠ¸ 1", "ì»¨í…ìŠ¤íŠ¸ 2"],
    ground_truth="AIëŠ”..."
)

print(f"Faithfulness: {rag_result.faithfulness:.4f}")
print(f"Answer Relevance: {rag_result.answer_relevance:.4f}")
```

---

## ğŸ› ï¸ ê³ ê¸‰ ê¸°ëŠ¥

### 1. Memory ì‚¬ìš©

```python
from llmkit import BufferMemory

memory = BufferMemory(max_messages=10)

# ëŒ€í™” ì¶”ê°€
memory.add_message("user", "ë‚´ ì´ë¦„ì€ í™ê¸¸ë™ì´ì•¼")
memory.add_message("assistant", "ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ë‹˜!")

# ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
history = memory.get_messages()
print(history)
```

### 2. Output Parsers

```python
from llmkit import PydanticOutputParser
from pydantic import BaseModel

class Person(BaseModel):
    name: str
    age: int

parser = PydanticOutputParser(pydantic_object=Person)

response = client.chat(
    "í™ê¸¸ë™, 30ì„¸ì— ëŒ€í•œ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”",
    output_parser=parser
)

person = response.parsed  # Person ê°ì²´
print(person.name, person.age)
```

### 3. Prompt Templates

```python
from llmkit import PromptTemplate, FewShotPromptTemplate

# ê¸°ë³¸ í…œí”Œë¦¿
template = PromptTemplate(
    template="{source}ì—ì„œ {target}ë¡œ ë²ˆì—­: {text}",
    input_variables=["source", "target", "text"]
)

prompt = template.format(
    source="ì˜ì–´",
    target="í•œêµ­ì–´",
    text="Hello"
)

# Few-shot í…œí”Œë¦¿
few_shot = FewShotPromptTemplate(
    examples=[
        {"input": "2+2", "output": "4"},
        {"input": "3*5", "output": "15"}
    ],
    example_template=PromptTemplate(
        template="Q: {input}\nA: {output}",
        input_variables=["input", "output"]
    ),
    prefix="ìˆ˜í•™ ë¬¸ì œë¥¼ í’€ì–´ì£¼ì„¸ìš”:",
    suffix="Q: {input}\nA:"
)
```

---

## ğŸ”§ ê°œë°œ ë„êµ¬

### Makefile ì‚¬ìš©

```bash
# ê°œë°œ ë„êµ¬ ì„¤ì¹˜
make install-dev

# ë¹ ë¥¸ ìë™ ìˆ˜ì •
make quick-fix

# íƒ€ì… ì²´í¬
make type-check

# ë¦°íŠ¸ ì²´í¬
make lint

# ì „ì²´ ê²€ì‚¬ ë° ìˆ˜ì •
make all
```

### Poetry ì‚¬ìš©

```bash
# ì˜ì¡´ì„± ì¶”ê°€
poetry add openai
poetry add --group dev pytest

# ì˜ì¡´ì„± ì—…ë°ì´íŠ¸
poetry update

# ê°€ìƒ í™˜ê²½ ì •ë³´
poetry env info
```

---

## ğŸ“š ë‹¤ìŒ ë‹¨ê³„

1. **ë¬¸ì„œ ì½ê¸°**: [`docs/`](docs/) í´ë”ì˜ ìƒì„¸ ë¬¸ì„œ
2. **ì˜ˆì œ ì‹¤í–‰**: [`examples/`](examples/) í´ë”ì˜ ì˜ˆì œ ì½”ë“œ
3. **íŠœí† ë¦¬ì–¼**: [`docs/tutorials/`](docs/tutorials/) í´ë”ì˜ íŠœí† ë¦¬ì–¼
4. **ì•„í‚¤í…ì²˜ ì´í•´**: [`ARCHITECTURE.md`](ARCHITECTURE.md) ì°¸ê³ 

---

## â“ ë¬¸ì œ í•´ê²°

### Providerë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ

```bash
# Provider ì„¤ì¹˜ í™•ì¸
poetry install --extras all
# ë˜ëŠ”
pip install llmkit[all]
```

### API í‚¤ ì˜¤ë¥˜

```bash
# .env íŒŒì¼ í™•ì¸
cat .env

# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
echo $OPENAI_API_KEY
```

### Import ì˜¤ë¥˜

```python
# ì˜¬ë°”ë¥¸ import ë°©ë²•
from llmkit import Client  # âœ…
# from llmkit.client import Client  # âŒ (êµ¬ë²„ì „)
```

---

**ë” ìì„¸í•œ ë‚´ìš©ì€ [README.md](README.md)ì™€ [ARCHITECTURE.md](ARCHITECTURE.md)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”!**
